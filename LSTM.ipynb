{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1fb0e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/direnceran/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.utils import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, GlobalMaxPooling1D, SpatialDropout1D\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf08d9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Tweet_ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will murder yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>I am coming to the borders and I will kill you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands and i will kill you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im coming on borderlands and i will murder you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2401</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71651</th>\n",
       "      <td>74677</td>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that the Windows partition of my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71652</th>\n",
       "      <td>74678</td>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized that my Mac window partition is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71653</th>\n",
       "      <td>74679</td>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized the windows partition of my Mac ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71654</th>\n",
       "      <td>74680</td>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just realized between the windows partition of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71655</th>\n",
       "      <td>74681</td>\n",
       "      <td>9200</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Just like the windows partition of my Mac is l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71656 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  Tweet_ID       Entity Sentiment  \\\n",
       "0          0      2401  Borderlands  Positive   \n",
       "1          1      2401  Borderlands  Positive   \n",
       "2          2      2401  Borderlands  Positive   \n",
       "3          3      2401  Borderlands  Positive   \n",
       "4          4      2401  Borderlands  Positive   \n",
       "...      ...       ...          ...       ...   \n",
       "71651  74677      9200       Nvidia  Positive   \n",
       "71652  74678      9200       Nvidia  Positive   \n",
       "71653  74679      9200       Nvidia  Positive   \n",
       "71654  74680      9200       Nvidia  Positive   \n",
       "71655  74681      9200       Nvidia  Positive   \n",
       "\n",
       "                                           Tweet_content  \n",
       "0      im getting on borderlands and i will murder yo...  \n",
       "1      I am coming to the borders and I will kill you...  \n",
       "2      im getting on borderlands and i will kill you ...  \n",
       "3      im coming on borderlands and i will murder you...  \n",
       "4      im getting on borderlands 2 and i will murder ...  \n",
       "...                                                  ...  \n",
       "71651  Just realized that the Windows partition of my...  \n",
       "71652  Just realized that my Mac window partition is ...  \n",
       "71653  Just realized the windows partition of my Mac ...  \n",
       "71654  Just realized between the windows partition of...  \n",
       "71655  Just like the windows partition of my Mac is l...  \n",
       "\n",
       "[71656 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('twitter_training.csv',sep=',', names=['Tweet_ID','Entity','Sentiment','Tweet_content']) #Verileri yükleyelim xd\n",
    "\n",
    "data.drop_duplicates(inplace=True) \n",
    "\n",
    "data.dropna(axis=0, inplace=True) \n",
    "data.reset_index(inplace=True) \n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07d5a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max Tweet_content len: 311\n"
     ]
    }
   ],
   "source": [
    "replace_list = {r\"i'm\": 'i am',\n",
    "                r\"'re\": ' are',\n",
    "                r\"let’s\": 'let us',\n",
    "                r\"'s\":  ' is',\n",
    "                r\"'ve\": ' have',\n",
    "                r\"can't\": 'can not',\n",
    "                r\"cannot\": 'can not',\n",
    "                r\"shan’t\": 'shall not',\n",
    "                r\"n't\": ' not',\n",
    "                r\"'d\": ' would',\n",
    "                r\"'ll\": ' will',\n",
    "                r\"'scuse\": 'excuse',\n",
    "                ',': ' ,',\n",
    "                '.': ' .',\n",
    "                '!': ' !',\n",
    "                '?': ' ?',\n",
    "                '\\s+': ' '}  \n",
    "\n",
    "\n",
    "def clean_text(text):  \n",
    "    text = text.lower()\n",
    "    for s in replace_list:\n",
    "        text = text.replace(s, replace_list[s])\n",
    "    text = ' '.join(text.split())\n",
    "    return text   \n",
    "\n",
    "x = data['Tweet_content'].apply(lambda p: clean_text(p))\n",
    "\n",
    "Tweet_content_len = x.apply(lambda p: len(p.split(' ')))\n",
    "max_Tweet_content_len = Tweet_content_len.max()\n",
    "print('max Tweet_content len: {0}'.format(max_Tweet_content_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7608b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REPLACE_WITH_SPACE = re.compile(\"(@)\")\n",
    "\n",
    "SPACE = \" \"\n",
    "\n",
    "\n",
    "english_stop_words = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5562047",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reviews(reviews):\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line.lower()) for line in reviews]\n",
    "    return reviews\n",
    "\n",
    "\n",
    "\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() if word not in english_stop_words]))\n",
    "    return removed_stop_words\n",
    "\n",
    "\n",
    "\n",
    "def get_stemmed_text(corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "\n",
    "\n",
    "y =data['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9623fe0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviewtweet = reviews(x)\n",
    "\n",
    "no_stop_words_tweet = remove_stop_words(reviewtweet)\n",
    "\n",
    "stemmed_reviews_tweet = get_stemmed_text(no_stop_words_tweet)\n",
    "\n",
    "max_words = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12330d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(  \n",
    "    num_words = max_words,\n",
    "    filters = '\"#$%&()*+-/:;<=>@[\\]^_`{|}~'\n",
    ")\n",
    "tokenizer.fit_on_texts(stemmed_reviews_tweet)\n",
    "x = tokenizer.texts_to_sequences(stemmed_reviews_tweet)\n",
    "x = pad_sequences(x, maxlen = 300)\n",
    "\n",
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(y)\n",
    "\n",
    "y = np.array(label_tokenizer.texts_to_sequences(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "002db87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 18:28:33.547490: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112/112 [==============================] - 319s 3s/step - loss: 1.2631 - accuracy: 0.4296 - val_loss: 1.0232 - val_accuracy: 0.5825\n",
      "Epoch 2/10\n",
      "112/112 [==============================] - 338s 3s/step - loss: 0.8839 - accuracy: 0.6521 - val_loss: 0.7717 - val_accuracy: 0.6961\n",
      "Epoch 3/10\n",
      "112/112 [==============================] - 335s 3s/step - loss: 0.6963 - accuracy: 0.7355 - val_loss: 0.6786 - val_accuracy: 0.7367\n",
      "Epoch 4/10\n",
      "112/112 [==============================] - 341s 3s/step - loss: 0.6040 - accuracy: 0.7720 - val_loss: 0.6527 - val_accuracy: 0.7494\n",
      "Epoch 5/10\n",
      "112/112 [==============================] - 341s 3s/step - loss: 0.5465 - accuracy: 0.7928 - val_loss: 0.6122 - val_accuracy: 0.7648\n",
      "Epoch 6/10\n",
      "112/112 [==============================] - 332s 3s/step - loss: 0.4956 - accuracy: 0.8121 - val_loss: 0.5881 - val_accuracy: 0.7797\n",
      "Epoch 7/10\n",
      "112/112 [==============================] - 343s 3s/step - loss: 0.4523 - accuracy: 0.8291 - val_loss: 0.5678 - val_accuracy: 0.7898\n",
      "Epoch 8/10\n",
      "112/112 [==============================] - 332s 3s/step - loss: 0.4191 - accuracy: 0.8390 - val_loss: 0.5637 - val_accuracy: 0.7896\n",
      "Epoch 9/10\n",
      "112/112 [==============================] - 334s 3s/step - loss: 0.3928 - accuracy: 0.8507 - val_loss: 0.5424 - val_accuracy: 0.8037\n",
      "Epoch 10/10\n",
      "112/112 [==============================] - 302s 3s/step - loss: 0.3670 - accuracy: 0.8595 - val_loss: 0.5596 - val_accuracy: 0.8054\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 128)          1024000   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 300, 128)         0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,172,741\n",
      "Trainable params: 1,172,741\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.8) \n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(input_dim = max_words, output_dim = 128, input_length = 300))\n",
    "model_lstm.add(SpatialDropout1D(0.3))\n",
    "model_lstm.add(LSTM(128, dropout = 0.3, recurrent_dropout = 0.3))\n",
    "model_lstm.add(Dense(128, activation = 'relu'))\n",
    "model_lstm.add(Dropout(0.3))\n",
    "model_lstm.add(Dense(5, activation = 'softmax'))\n",
    "model_lstm.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='Adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model_lstm.fit( \n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=(x_test,y_test) ,\n",
    "    epochs = 10,\n",
    "    batch_size = 512\n",
    ")\n",
    "\n",
    "model_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf371b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
